{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding Gene Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I build an autoencoder that takes ~19,000 protein encoding gene expression values as input, and encodes it to varying (much smaller) dimensions and then decodes them back to the original width of the input. \n",
    "\n",
    "I first normalize the data using the l2 norm after splitting the data into training (67%) and testing sets. Later, I build an autoencoder with 7 layers and 50 dimensions in the bottleneck layer. I use `adam` as optimizer and `binary crossentropy` loss function to compare the results of the decoder with the original input. The model converges at loss = 0.0041 which indicates a fairly high quality compression by the encoder.\n",
    "\n",
    "I later experiment with different sizes for bottleneck layer and try 10, 20, 30, 40, 60, and 80 to document the change in loss. As expected, we get the lowest loss on our validation set when the bottleneck layer is the widest, but the difference is very small. It is expected to get higher loss when the bottleneck is narrower, as we are forcing data to a lower dimension which causes a higher decrease in information loss. \n",
    "\n",
    "Next, I vary the depth of the network to see its impact on loss. The model with just one hidden layer converges later and at a higher loss value compared to the model with 15 hidden layers. However, the training time of the shallow model is considerably less. \n",
    "\n",
    "Using autoencoders on gene expression data can help us create clusters of different types of cancer as we force it to a lower dimension representation, analogous to more traditional unsupervised learning algorithms. It can also help generate synthetic data highly is very valuable due to relatively higher costs and of RNA sequencing as well as numerous barriers in front of researchers to access resources to collect and construct organic datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_coding = pd.read_csv('data/nt.coding.csv')\n",
    "nt_coding.drop('Type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to fit data into [0, 1] scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(nt_coding, test_size=0.33, random_state=1)\n",
    "\n",
    "scaler = Normalizer()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoded = Dense(256, activation='relu')(input_layer)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 3s 40ms/step - loss: 0.5272\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0135\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0055\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0047\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0043\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0043\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0042\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0042\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 46ms/step - loss: 0.0041\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa52b1b3400>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=20,\n",
    "                batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 0s - loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "loss = autoencoder.evaluate(X_test, X_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different bottleneck sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.5632\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0116\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0060\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0058\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0052\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0046\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0043\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0041\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0041\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0041\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0042\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0041\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0041\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "15/15 - 0s - loss: 0.0041\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.5819\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0135\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0067\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0051\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0045\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0043\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0043\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0041\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0041\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0041\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 36ms/step - loss: 0.0042\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0043\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0040\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0041\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0042\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0041\n",
      "15/15 - 0s - loss: 0.0041\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.5402\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0140\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0066\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0057\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0048\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0043\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0043\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0043\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0041\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0041\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0042\n",
      "15/15 - 0s - loss: 0.0041\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.5638\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0136\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0063\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 4s 92ms/step - loss: 0.0049\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 49ms/step - loss: 0.0044\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0043\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0042\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 46ms/step - loss: 0.0041\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0042\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0041\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0041\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0041\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 45ms/step - loss: 0.0042\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 47ms/step - loss: 0.0042\n",
      "15/15 - 0s - loss: 0.0041\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 3s 50ms/step - loss: 0.5347\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 45ms/step - loss: 0.0142\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 48ms/step - loss: 0.0057\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 2s 45ms/step - loss: 0.0047\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 49ms/step - loss: 0.0044\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 45ms/step - loss: 0.0043\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0043\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0042\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0041\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 47ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 48ms/step - loss: 0.0043\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 46ms/step - loss: 0.0041\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 46ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 43ms/step - loss: 0.0041\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0042\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 44ms/step - loss: 0.0042\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 42ms/step - loss: 0.0041\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0042\n",
      "15/15 - 0s - loss: 0.0041\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 41ms/step - loss: 0.5355\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0140\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0062\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0049\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0043\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0043\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0043\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0041\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0041\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 2s 37ms/step - loss: 0.0042\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0042\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 2s 41ms/step - loss: 0.0041\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0042\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0041\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 2s 38ms/step - loss: 0.0041\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.0042\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 2s 40ms/step - loss: 0.0041\n",
      "15/15 - 0s - loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "loss_dict = dict()\n",
    "del autoencoder, encoded, decoded\n",
    "for size in [10, 20, 30, 40, 60, 80]:\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    encoded = Dense(256, activation='relu')(input_layer)\n",
    "    encoded = Dense(128, activation='relu')(encoded)\n",
    "    encoded = Dense(size, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(128, activation='relu')(encoded)\n",
    "    decoded = Dense(256, activation='relu')(decoded)\n",
    "    decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                epochs=20,\n",
    "                batch_size=20)\n",
    "    loss_dict[size] = autoencoder.evaluate(X_test, X_test, verbose=2)\n",
    "    del autoencoder, encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the lowest loss on our validation set when the bottleneck layer is the widest, but the difference is very small. It is expected to get higher loss when the bottleneck is narrower, as we are forcing data to a lower dimension which causes a higher decrease in information loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.004094595089554787,\n",
       " 20: 0.004099169746041298,\n",
       " 30: 0.0040933662094175816,\n",
       " 40: 0.00409733084961772,\n",
       " 60: 0.004088591318577528,\n",
       " 80: 0.004073957446962595}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 1s 10ms/step - loss: 0.6747\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.2982\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0431\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0182\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0120\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0090\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0075\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0069\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0065\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0062\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0061\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0058\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0057\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0054\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0055\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0055\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0053\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0052\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0051\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa540276f40>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoded = Dense(50, activation='relu')(input_layer)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=20,\n",
    "                batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 0s - loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004860951565206051"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.evaluate(X_test, X_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 41s 809ms/step - loss: 0.3867\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 37s 799ms/step - loss: 0.0048\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 42s 900ms/step - loss: 0.0042\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 40s 850ms/step - loss: 0.0042\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 39s 826ms/step - loss: 0.0042\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 38s 798ms/step - loss: 0.0042\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 38s 805ms/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 37s 790ms/step - loss: 0.0042\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 36s 769ms/step - loss: 0.0041\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 37s 783ms/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 37s 777ms/step - loss: 0.0043\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 36s 764ms/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 36s 763ms/step - loss: 0.0042\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 36s 763ms/step - loss: 0.0042\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 41s 867ms/step - loss: 0.0042\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 39s 833ms/step - loss: 0.0042\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 41s 879ms/step - loss: 0.0042\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 40s 855ms/step - loss: 0.0042\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 36s 775ms/step - loss: 0.0042\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 36s 763ms/step - loss: 0.0042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa54092f550>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoded = Dense(5000, activation='relu')(input_layer)\n",
    "encoded = Dense(2048, activation='relu')(encoded)\n",
    "encoded = Dense(1024, activation='relu')(encoded)\n",
    "encoded = Dense(512, activation='relu')(encoded)\n",
    "encoded = Dense(256, activation='relu')(encoded)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(512, activation='relu')(decoded)\n",
    "decoded = Dense(1024, activation='relu')(decoded)\n",
    "decoded = Dense(2048, activation='relu')(decoded)\n",
    "decoded = Dense(5000, activation='relu')(decoded)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=20,\n",
    "                batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 2s - loss: 0.0041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00411525834351778"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.evaluate(X_test, X_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher depth model takes much longer to train but converges faster at lower loss. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
